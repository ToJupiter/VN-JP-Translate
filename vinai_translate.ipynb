{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "!pip install datasets --upgrade transformers pandas psutil polars protobuf tiktoken blobfile sentencepiece accelerate==0.26.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "\n",
    "# Define output paths\n",
    "OUTPUT_DIR = \"/content/batches\"\n",
    "FINAL_OUTPUT_PATH = \"/content/final_translations.csv\"\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Monitor memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class BatchTranslator:\n",
    "    def __init__(self, model_name=\"vinai/vinai-translate-en2vi-v2\", device=None, batch_size=8):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"en_XX\")\n",
    "        self.load_model(model_name)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        \"\"\"Load the model with memory efficiency settings\"\"\"\n",
    "        print(f\"Loading model: {model_name} to {self.device}\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        ).to(self.device)\n",
    "        print(f\"Model loaded. Memory usage: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unload model from GPU memory\"\"\"\n",
    "        print(\"Unloading model...\")\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        clean_memory()\n",
    "        print(f\"Model unloaded. Memory usage after cleanup: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "    def translate_batch(self, batch_en_passages):\n",
    "        \"\"\"Translates a batch of English passages to Vietnamese using the vinai model.\"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            self.load_model(self.model_name)\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                batch_en_passages,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    do_sample=True,\n",
    "                    top_k=100,\n",
    "                    top_p=0.8,\n",
    "                    decoder_start_token_id=self.tokenizer.lang_code_to_id[\"vi_VN\"],\n",
    "                    max_length=512,\n",
    "                    num_return_sequences=1,\n",
    "                )\n",
    "\n",
    "            translations = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            cleaned_translations = [t.strip() for t in translations]\n",
    "\n",
    "            del inputs, output_ids\n",
    "            clean_memory()\n",
    "\n",
    "            return cleaned_translations\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Runtime Error during translation: {e}\")\n",
    "            clean_memory()\n",
    "            return [f\"Translation Error: {e}\"] * len(batch_en_passages)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected Error during translation: {e}\")\n",
    "            clean_memory()\n",
    "            return [f\"Translation Error: {e}\"] * len(batch_en_passages)\n",
    "\n",
    "def process_dataset_in_batches(df, batch_size, st, e):\n",
    "    \"\"\"Processes the dataset in batches and translates each batch\"\"\"\n",
    "    print(f\"Processing rows {st} to {e} with batch size {batch_size}...\")\n",
    "    translator = BatchTranslator()\n",
    "    \n",
    "    for start in range(st, e, batch_size):\n",
    "        end = min(start + batch_size, e)\n",
    "        batch_data = df[start:end]['en_passages'].to_list()\n",
    "        translations = translator.translate_batch(batch_data)\n",
    "        print(f\"Translated rows {start} to {end}\")\n",
    "    \n",
    "    translator.unload_model()\n",
    "\n",
    "def combine_batch_files():\n",
    "    \"\"\"Combines batch results into the final output file\"\"\"\n",
    "    print(\"Combining batch files into final output...\")\n",
    "    # Example logic: Append all batch results into one file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    try:\n",
    "        jp_path_pattern = 'hf://datasets/hotchpotch/ms_marco_japanese/v2.1-madlad400-3b/train-*.parquet'\n",
    "        en_path = 'hf://datasets/microsoft/ms_marco/v2.1/train-*.parquet'\n",
    "\n",
    "        jp_df = pl.read_parquet(jp_path_pattern)\n",
    "        en_df = pl.read_parquet(en_path)\n",
    "\n",
    "        print(f\"Loaded Japanese data shape: {jp_df.shape}\")\n",
    "        print(f\"Loaded English data shape: {en_df.shape}\")\n",
    "\n",
    "        jp_df = jp_df.select(['query_id', pl.col('passages').struct.field('passage_text').alias('jp_passages')])\n",
    "        en_df = en_df.select(['query_id', pl.col('passages').struct.field('passage_text').alias('en_passages')])\n",
    "\n",
    "        print(\"Joining datasets on query_id...\")\n",
    "        merged = jp_df.join(en_df, on=\"query_id\", how=\"inner\")\n",
    "\n",
    "        print(f\"Merged data shape: {merged.shape}\")\n",
    "        if merged.is_empty():\n",
    "            raise ValueError(\"Merged DataFrame is empty. Check join key 'query_id' and input data.\")\n",
    "\n",
    "        merged = merged.explode(['en_passages'])\n",
    "        print(f\"Exploded data shape (one row per passage): {merged.shape}\")\n",
    "        merged = merged.filter(pl.col('en_passages') != \"\")\n",
    "\n",
    "        print(f\"Shape after filtering empty passages: {merged.shape}\")\n",
    "        print(\"Sample of merged data:\")\n",
    "        print(merged.head(5))\n",
    "\n",
    "        START_ROW = 10000\n",
    "        END_ROW = 10000 + 1024\n",
    "        BATCH_SIZE = 128\n",
    "\n",
    "        process_dataset_in_batches(merged, batch_size=BATCH_SIZE, st=START_ROW, e=END_ROW)\n",
    "        combine_batch_files()\n",
    "\n",
    "    except Exception as main_e:\n",
    "        print(f\"An error occurred in the main execution block: {main_e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"Script finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
